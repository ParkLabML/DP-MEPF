# dp-mepf code

## code structure
- `models/`
  - `model_builder.py` gives access to all NNs used in the code
  - definitions for all models
  - place to put pretrained models obtained from the GFMN paper
- `data_loading.py` prepares the dataloader
- `downstream_eval.py` cifar10 classification
- `dp_analysis.py` functions to get Gaussian mechanism noise variables based on desired eps,delta guarantee
- `dp_functions.py` everything related to sensitivity bounding and DP release
- `dp_mepf.py` This is the main function to run. read through main() to get an idea of the structure
- `dp_mepf_args.py` all the flags that may be set for dp_mepf.py (includes unused features)
- `dp_merf_cifar.py` cifar10 dp-merf baseline
- `feature_matching.py` functions used for feature matching updates (regular or adam)
- `fid_eval.py` FID score computation
- `util.py` various utility functions
- `util_logging.py` utility functions specific to logging experiments


## How to run:

### 1) notable flags:
- `--seed`: value of random seed to use. we chose 1, 2, and 3
- `--matched_moments`: either `mean` or `m1_and_m2` to indicate whether we match phi_1 or both phi_1 and phi_2.
- `--tgt-eps`: the desired DP constant epsilon. the necessary noise is computed.
- `--no_io_files`: by default, io is routed to files in the log directory. this flag disables this
- `--restart_iter`: by default, program exits with exit code 3 after this many iterations to trigger a restart on the cluster we used for training. Set to None or negative number to turn off.
- `--pytorch_encoders`: uses models from the torchvision library rather than those from the Paper "Learning Implicit Generative Models by Matching Perceptual Features" by dos Santos et al. This is used for all Cifar10 and CelebA experiments

### 2) Run via command line

all experiments were performed with random seeds `1`,`2` and `3` (and `4`, `5` for CelebA and Cifar10), for matching only phi_1 or phi_1 and phi_2, and at various levels of privacy.
Below we only list one nonprivate and one private run for each dataset. the other runs can be generated by changing the `seed`, `matched_moments` and `tgt_eps` arguments accordingly.

#### a) MNIST

non-DP
`python3 dp_mepf.py  --dataset dmnist --syn_eval_iter 20_000 --labeled --net_enc_type resnet18 --batch_size 100 --downstream_dataset_size 60_000 --exp_name may11_dmnist_nondp/run_0 --matched_moments mean --lr 1e-5 --m_avg_lr 1e-3 --manual_seed 1`

DP
`python3 dp_mepf.py  --dataset dmnist --syn_eval_iter 20_000 --labeled --net_enc_type resnet18 --batch_size 100 --downstream_dataset_size 60_000 --exp_name may12_dmnist_private_res/run_0 --matched_moments mean --dp_tgt_eps 0.2 --dp_mean_bound 1. --dp_var_bound 1. --lr 1e-5 --m_avg_lr 1e-4 --manual_seed 1`




#### b) FashionMNIST

non-DP
`python3 dp_mepf.py  --dataset fmnist --syn_eval_iter 20_000 --labeled --net_enc_type resnet18 --batch_size 100 --downstream_dataset_size 60_000 --exp_name may12_fmnist_nondp_res/run_0 --matched_moments mean --lr 1e-5 --m_avg_lr 1e-3 --manual_seed 1`

DP
`python3 dp_mepf.py  --dataset fmnist --syn_eval_iter 20_000 --labeled --net_enc_type resnet18 --batch_size 100 --downstream_dataset_size 60_000 --exp_name may12_fmnist_dp_res/run_0 --matched_moments mean --dp_tgt_eps 0.2 --dp_mean_bound 1. --dp_var_bound 1. --lr 1e-4 --m_avg_lr 1e-3 --manual_seed 1`


#### c) CelebA 32x32
nonDP
`python3.9 dp_mepf.py --pytorch_encoders --n_iter 200_000 --valid_iter 5_000 --restart_iter 5_000 --syn_eval_iter 5_000 --dataset celeba --val_enc fid_features --val_data train --validation_mode static --exp_name celeba_results_nondp/run_0 --dp_tgt_eps 1000. --matched_moments m1_and_m2 --dp_val_noise_scaling 10. --batch_size 128 --lr 1e-3 --m_avg_lr 1e-4 --seed 1`

DP
`python3.9 dp_mepf.py --pytorch_encoders --n_iter 200_000 --valid_iter 5_000 --restart_iter 5_000 --syn_eval_iter 5_000 --dataset celeba --val_enc fid_features --val_data train --validation_mode static --exp_name celeba_results_dp/run_0 --dp_tgt_eps 0.2 --matched_moments m1_and_m2 --dp_val_noise_scaling 10. --batch_size 128 --lr 1e-3 --m_avg_lr 1e-4 --seed 1`

#### c) CelebA 64x64
nonDP
`python3.9 dp_mepf.py --pytorch_encoders --n_iter 200_000 --valid_iter 5_000 --restart_iter 5_000 --syn_eval_iter 5_000 --dataset celeba --image_size 64 --val_enc fid_features --val_data train --validation_mode static --exp_name celeba_64x64_results_nondp/run_0 --dp_tgt_eps 1000. --matched_moments m1_and_m2 --dp_val_noise_scaling 10. --batch_size 128 --lr 1e-4 --m_avg_lr 3e-4 --seed 1`

DP
`python3.9 dp_mepf.py --pytorch_encoders --n_iter 200_000 --valid_iter 5_000 --restart_iter 5_000 --syn_eval_iter 5_000 --dataset celeba --image_size 64 --val_enc fid_features --val_data train --validation_mode static --exp_name celeba_64x64_results_dp/run_0 --dp_tgt_eps 0.2 --matched_moments m1_and_m2 --dp_val_noise_scaling 10. --batch_size 128 --lr 1e-4 --m_avg_lr 3e-4 --seed 1`


#### d) Cifar10 - labeled

nonDP
`python3.9 dp_mepf.py --pytorch_encoders --n_iter 200_000 --valid_iter 5_000 --restart_iter 5_000 --syn_eval_iter 5_000 --dataset cifar10 --labeled --val_enc fid_features --val_data train --validation_mode static --exp_name cifar10_labeled_results_nondp/run_0 --dp_tgt_eps 1000. --matched_moments m1_and_m2 --dp_val_noise_scaling 10. --batch_size 64 --lr 1e-3 --m_avg_lr 1e-2 --seed 1`

DP
`python3.9 dp_mepf.py --pytorch_encoders --n_iter 200_000 --valid_iter 5_000 --restart_iter 5_000 --syn_eval_iter 5_000 --dataset cifar10 --labeled --val_enc fid_features --val_data train --validation_mode static --exp_name cifar10_labeled_results_dp/run_0 --dp_tgt_eps 2. --matched_moments m1_and_m2 --dp_val_noise_scaling 10. --batch_size 64 --lr 1e-3 --m_avg_lr 1e-2 --seed 1`


#### e) Cifar10 - unlabeled

non-DP
`python3.9 dp_mepf.py --pytorch_encoders --n_iter 200_000 --valid_iter 5_000 --restart_iter 5_000 --syn_eval_iter 5_000 --dataset cifar10 --val_enc fid_features --val_data train --validation_mode static --exp_name cifar10_results_nondp/run_0 --dp_tgt_eps 1000. --matched_moments m1_and_m2 --dp_val_noise_scaling 10. --batch_size 128 --lr 1e-3 --m_avg_lr 3e-4 --seed 1`

DP
`python3.9 dp_mepf.py --pytorch_encoders --n_iter 200_000 --valid_iter 5_000 --restart_iter 5_000 --syn_eval_iter 5_000 --dataset cifar10 --val_enc fid_features --val_data train --validation_mode static --exp_name cifar10_results_dp/run_0 --dp_tgt_eps 0.2 --matched_moments m1_and_m2 --dp_val_noise_scaling 10. --batch_size 128 --lr 1e-3 --m_avg_lr 3e-4 --seed 1`


#### f) DP-MERF

non-DP
`python3 dp_merf_cifar.py  --log-interval 1000 --d-code 90 --log-name dpmerf_res/run_0 --batch-size 64 --d-rff 50000 --rff-sigma 1000. --lr 1e-3 --scheduler-interval 10000   --seed 1`

DP
`python3 dp_merf_cifar.py  --log-interval 1000 --d-code 90 --log-name dpmerf_res/run_9 --batch-size 64 --d-rff 50000 --rff-sigma 1000. --lr 1e-3 --scheduler-interval 10000  --tgt-eps 2 --seed 1`


#### g) DP-GAN

Imagenet pre-training
`python3.9 dcgan_baseline_backpack.py --model resnet --data imagenet32  --exp_name sep28_dcgan_resnet_bp_imagenet32/run_1 --batch_size 64 --lr_gen 3e-5 --lr_dis 1e-5 --n_epochs 30`

CelebA
`python3.9 dcgan_baseline_backpack.py --model resnet --data celeba --local_data --pretrain_exp sep28_dcgan_resnet_bp_imagenet32/run_1 --exp_name dcgan_celeba_res/run_0 --batch_size 512 --lr_gen 3e-4 --lr_dis 3e-4 --n_epochs 10 -gen_freq 10 --clip_norm 1e-5 --target_eps 0.2 --seed 1`

Cifar10
`python3.9 dcgan_baseline_backpack.py --model resnet --data cifar10 --batch_size_grad_acc 64 --local_data --pretrain_exp sep28_dcgan_resnet_bp_imagenet32/run_1 --exp_name dpgan_cifar10_res/run_0 --batch_size 512 --lr_gen 1e-4 --lr_dis 1e-3 --n_epochs 10 -gen_freq 10 --clip_norm 1e-5 --target_eps 0.2 --seed 1`
